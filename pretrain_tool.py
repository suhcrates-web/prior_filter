import torch
import typing
import os
import transformers
from transformers import AutoConfig, AutoModelForCausalLM, GPT2TokenizerFast

import torch.nn.functional as F
import torchmetrics
import torch.nn as nn
from tqdm import tqdm
from collections import OrderedDict

class NLL(torchmetrics.aggregation.MeanMetric):
  pass

class Perplexity(NLL):
  def compute(self): # -> Tensor
    """Computes the Perplexity.

    Returns:
     Perplexity
    """
    return torch.exp(self.mean_value / self.weight)


class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        self.config = config
        self.model_config =  AutoConfig.from_pretrained(config.base_model_name)
        self.model_config.torch_dtype = torch.bfloat16
        self.backbone =  AutoModelForCausalLM.from_config(self.model_config)
        self.backbone.to(dtype=torch.bfloat16)
        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2-large', padding=True, truncation=True, return_tensors='pt')
        self.tokenizer.pad_token = self.tokenizer.eos_token 
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        self.trainer = None
        self.device = 'cpu'
        self.eval_model_tokenizer = self.tokenizer
        
        self.gen_ppl_metric = Perplexity()
        self.loss_metric = NLL()
        self.valid_loss_metric = NLL()
        
    def _compute_loss(self, inputs):
        if 'labels' not in inputs:
            inputs['labels'] = inputs['input_ids'].clone()
        logits = self.backbone.forward(**inputs)
        return logits.loss



    def eval_step(self, rank, world_size, dataloader_valid, global_step, epoch, ):
        self.backbone.eval()
        if rank==0:
            print("eval 시작")
            
        #### gen ppl ####    
        if rank==0:
            print('!!!! gen ppl!!! ')
        inputs = self.tokenizer(['<|endoftext|>'] *4 , return_tensors='pt', padding=True, truncation=True)
        inputs = {k: v.to(rank, non_blocking=True) for k, v in inputs.items()}
        out = self.backbone.generate(**inputs, max_length=512, do_sample=True)
        self.compute_generative_perplexity(out)
        gen_ppl = self.gen_ppl_metric.compute()
        text_samples = self.tokenizer.batch_decode(out)
        if rank==0:
            self.trainer.logger.log_table(
                key=f'samples@global_step{global_step}_{rank}',
                columns=['Generated Samples'],
                data=[[s] for s in text_samples])
        
        tqdm_disable = False if rank==0 else True
        total_steps = len(dataloader_valid)
        with torch.no_grad():
            self.backbone.eval()
            loss_list = []
            
            for i, batch_data in tqdm(enumerate(dataloader_valid), disable=tqdm_disable, total=total_steps):
                batch_data = {k: v.to(rank, non_blocking=True) for k, v in batch_data.items()}
                loss = self._compute_loss(batch_data)
                
                loss_list.append(loss)
                self.valid_loss_metric.update(loss, torch.tensor(1.))
            
            logged_metrics = {}
            logged_metrics['global_step'] = global_step
            logged_metrics['epoch'] = epoch
            with torch.no_grad():
                loss_mean = torch.tensor(loss_list).mean().item()
            logged_metrics['val/loss'] = loss_mean 
            logged_metrics['val/loss_metric'] = self.valid_loss_metric.compute() 
            logged_metrics['val/gen_ppl'] = gen_ppl 
            self.trainer.logger.log_metrics(logged_metrics)
            self.valid_loss_metric.reset()
            
        if rank==0:
            ## save as model
            output_dir = f'{self.config.output_dir}/{self.config.wandb.name}/cp_ep{epoch}_gstep{global_step}'
            self.backbone.save_pretrained(output_dir)
            self.tokenizer.save_pretrained(output_dir)
            
            ## save as state_dict
            # state_dict = OrderedDict()
            # for name, param in self.backbone.named_parameters():
            #     if param.requires_grad:
            #         state_dict[name] = param.clone().detach().cpu()
            # ## save
            # result_dic = {'config': {** self.config},
            # }
            # result_dic['config']['world_size'] = world_size
            # result_dic['state_dict']=state_dict

            # torch.save(result_dic, f'{self.config.output_dir}/{self.config.wandb.name}/cp_ep{epoch}_gstep{global_step}.ckpt')



    @torch.no_grad()
    def compute_generative_perplexity(
        self,
        text_samples: typing.List[str],
        retokenize: bool = False,
        max_length: typing.Optional[int] = None,
        ) -> None:
        """Compute the generative perplexity of the model.

        Args:
            text_samples: List of sentences generated by the model.
        
        Returns:
            Perplexity of the generated text under a different
            pre-trained AR model (e.g., GPT2).
        """
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        eval_model = transformers.AutoModelForCausalLM.from_pretrained(
        'gpt2-large').to(text_samples.device).eval()

        # if retokenize:
        #   (samples, attn_mask,
        #    eval_context_size) = self.eval_retokenize(
        #      text_samples, max_length=max_length)
        # else:
        samples = text_samples
        attn_mask = torch.ones(samples.shape).to(samples.device)
        eval_context_size = samples.shape[-1]
        
        
        # batch_size = min(
        #   self.config.eval.perplexity_batch_size,
        #   samples.shape[0])
        batch_size = samples.shape[0]
        num_batches = samples.shape[0] // batch_size
        for i in range(num_batches):
            _samples = torch.split(
                samples[i * batch_size: (i + 1) * batch_size],
                eval_context_size,
                dim=-1)
            _attn_mask = torch.split(
                attn_mask[i * batch_size: (i + 1) * batch_size],
                eval_context_size,
                dim=-1)
            for (sample_chunk, attn_mask_chunk) in zip(
                _samples, _attn_mask):
                logits = eval_model(
                sample_chunk, attention_mask=attn_mask_chunk)[0]
                logits = logits.transpose(-1, -2)
                
                nlls = F.cross_entropy(logits[..., :-1],
                                    sample_chunk[..., 1:],
                                    reduction='none')
                first_eos = (sample_chunk == self.eval_model_tokenizer\
                            .eos_token_id).cumsum(-1) == 1
                token_mask = (
                sample_chunk
                != self.eval_model_tokenizer.eos_token_id)
                self.gen_ppl_metric.update(
                nlls, first_eos[..., 1:] + token_mask[..., 1:])
            
        # return nlls